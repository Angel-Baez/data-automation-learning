# ğŸ“Š Sistema de Procesamiento CSV Modular

Sistema avanzado de procesamiento de archivos CSV con arquitectura modular, validaciÃ³n de datos y configuraciÃ³n flexible. Proyecto de la **Semana 3** del programa Data + Automation Engineer Journey.

## ğŸ¯ CaracterÃ­sticas Principales

### âœ¨ Funcionalidades Core

- **Procesamiento Inteligente**: DetecciÃ³n automÃ¡tica de encoding, delimitadores y tipos de datos
- **ValidaciÃ³n Avanzada**: Sistema completo de validaciÃ³n con reglas personalizables
- **Arquitectura Modular**: Componentes independientes y reutilizables
- **ConfiguraciÃ³n Flexible**: GestiÃ³n centralizada con archivos .env y JSON
- **Logging Profesional**: Sistema de logs estructurado con mÃºltiples niveles
- **Testing Completo**: Suite de tests unitarios con >95% cobertura

### ğŸ”§ Capacidades TÃ©cnicas

- Manejo de archivos CSV de cualquier tamaÃ±o
- Transformaciones automÃ¡ticas de datos
- Limpieza y normalizaciÃ³n inteligente
- GeneraciÃ³n de reportes de calidad
- EstadÃ­sticas detalladas de procesamiento
- Sistema de backup automÃ¡tico

## ğŸ—ï¸ Arquitectura del Sistema

```
src/
â”œâ”€â”€ config_manager.py     # GestiÃ³n centralizada de configuraciÃ³n
â”œâ”€â”€ csv_processor.py      # Procesamiento principal de CSV
â”œâ”€â”€ data_validator.py     # Sistema de validaciÃ³n de datos
â””â”€â”€ utils.py             # Utilidades y helpers comunes

tests/                   # Suite completa de tests unitarios
config/                  # Archivos de configuraciÃ³n
data/                   # Directorio para datos de entrada/salida
logs/                   # Archivos de log del sistema
```

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Prerrequisitos

- Python 3.8+
- pip (gestor de paquetes de Python)

### InstalaciÃ³n

1. **Clonar o descargar el proyecto**

```bash
git clone <repository-url>
cd semana-03-csv-modular
```

2. **Crear entorno virtual (recomendado)**

```bash
python -m venv .venv

# En macOS/Linux:
source .venv/bin/activate

# En Windows:
.venv\Scripts\activate
```

3. **Instalar dependencias**

```bash
pip install -r requirements.txt
```

### ConfiguraciÃ³n Inicial

1. **Variables de entorno** (opcional)

```bash
# Crear archivo .env en la raÃ­z del proyecto
cp .env.example .env

# Editar .env con tus configuraciones:
LOG_LEVEL=INFO
CSV_BATCH_SIZE=10000
ENABLE_BACKUPS=true
MAX_FILE_SIZE_MB=100
```

2. **ConfiguraciÃ³n de procesamiento** (opcional)

```bash
# El sistema crearÃ¡ automÃ¡ticamente processing_rules.json
# Puedes personalizarlo segÃºn tus necesidades
```

## ğŸ“– Uso del Sistema

### Uso BÃ¡sico con Script Principal

```bash
# Procesar un archivo CSV
python main.py data/input/mi_archivo.csv

# Procesar con archivo de salida especÃ­fico
python main.py data/input/ventas.csv --output data/output/ventas_procesadas.csv

# Usar perfil de procesamiento especÃ­fico
python main.py data/input/datos.csv --profile strict
```

### Uso ProgramÃ¡tico

```python
from src.config_manager import ConfigManager
from src.csv_processor import CSVProcessor
from src.data_validator import DataValidator

# Inicializar sistema
config = ConfigManager()
processor = CSVProcessor(config)
validator = DataValidator(config)

# Procesar archivo
df, stats, metadata = processor.process_file("datos.csv")

# Validar datos
report = validator.validate_dataframe(df)

print(f"Procesadas {stats.final_rows} filas")
print(f"Calidad de datos: {report.success_rate:.1f}%")
```

### Ejemplos Avanzados

#### Procesamiento por Lotes

```python
import glob
from pathlib import Path

# Procesar mÃºltiples archivos
csv_files = glob.glob("data/input/*.csv")

for file_path in csv_files:
    try:
        df, stats, metadata = processor.process_file(file_path)
        print(f"âœ… {file_path}: {stats.final_rows} filas procesadas")
    except Exception as e:
        print(f"âŒ Error en {file_path}: {e}")
```

#### ValidaciÃ³n Personalizada

```python
from src.data_validator import ValidationRule, ValidationType, ValidationLevel

# Crear regla personalizada
age_rule = ValidationRule(
    name="age_validation",
    description="Edad debe estar entre 18 y 100",
    validation_type=ValidationType.RANGE_CHECK,
    level=ValidationLevel.ERROR,
    column="edad",
    parameters={'min': 18, 'max': 100}
)

validator.add_validation_rule(age_rule)
report = validator.validate_dataframe(df)
```

## ğŸ§ª Testing

Ejecutar la suite completa de tests:

```bash
# Todos los tests
python -m pytest tests/ -v

# Tests con cobertura
python -m pytest tests/ --cov=src --cov-report=html

# Tests especÃ­ficos
python -m pytest tests/test_csv_processor.py -v
```

### Estructura de Tests

- `test_config_manager.py`: Tests de configuraciÃ³n
- `test_csv_processor.py`: Tests de procesamiento CSV
- `test_data_validator.py`: Tests de validaciÃ³n
- `test_utils.py`: Tests de utilidades

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Archivo .env

```env
# ConfiguraciÃ³n de logging
LOG_LEVEL=INFO
LOG_DIR=logs

# ConfiguraciÃ³n de procesamiento
CSV_BATCH_SIZE=10000
MAX_FILE_SIZE_MB=100
ENABLE_BACKUPS=true

# ConfiguraciÃ³n de validaciÃ³n
VALIDATION_ENABLED=true
STRICT_MODE=false
```

### processing_rules.json

```json
{
  "processing_rules": {
    "remove_duplicates": true,
    "fill_missing_values": true,
    "normalize_text": true,
    "convert_data_types": true
  },
  "output_rules": {
    "save_metadata": true,
    "create_summary": true,
    "generate_report": true
  }
}
```

## ğŸ“Š Ejemplos de Datos

### CSV de Entrada (ejemplo)

```csv
id,nombre,edad,salario,fecha_ingreso
1,"Juan PÃ©rez",25,50000,2023-01-15
2,"MarÃ­a GarcÃ­a",32,65000,2022-11-20
3,"Pedro LÃ³pez",28,,2023-03-10
```

### Salida Procesada

- **Archivo CSV limpio** con datos normalizados
- **Reporte de calidad** con estadÃ­sticas detalladas
- **Metadatos** del procesamiento
- **Logs** del proceso completo

## ğŸ” Monitoreo y Logs

El sistema genera logs estructurados en mÃºltiples niveles:

```
2025-10-15 10:30:15 - INFO - Iniciando procesamiento de datos.csv
2025-10-15 10:30:16 - DEBUG - Detectado encoding: utf-8, delimitador: ,
2025-10-15 10:30:17 - WARNING - 5 filas con valores faltantes en columna 'salario'
2025-10-15 10:30:18 - INFO - Procesamiento completado: 1000 â†’ 995 filas
```

## ğŸ› ï¸ Desarrollo y ContribuciÃ³n

### Estructura del CÃ³digo

- **Tipo hints** completos en todo el cÃ³digo
- **Docstrings** en formato Google Style
- **Tests unitarios** para cada componente
- **Logging profesional** con contexto

### ExtensiÃ³n del Sistema

```python
# Agregar nueva transformaciÃ³n
def mi_transformacion_personalizada(df: pd.DataFrame) -> pd.DataFrame:
    # Tu lÃ³gica aquÃ­
    return df

# Registrar en el procesador
processor.add_custom_transformation("mi_transformacion", mi_transformacion_personalizada)
```

## ğŸ“ˆ Rendimiento

### Benchmarks TÃ­picos

- **Archivos pequeÃ±os** (<1MB): ~0.1-0.5 segundos
- **Archivos medianos** (1-10MB): ~0.5-2 segundos
- **Archivos grandes** (10-100MB): ~2-10 segundos
- **Memoria utilizada**: ~2-3x el tamaÃ±o del archivo

### Optimizaciones Implementadas

- Procesamiento por chunks para archivos grandes
- DetecciÃ³n eficiente de encoding y delimitadores
- Caching de configuraciones frecuentes
- LiberaciÃ³n automÃ¡tica de memoria

## ğŸ“ Contexto Educativo

Este proyecto forma parte del **programa Data + Automation Engineer Journey**, especÃ­ficamente de la **Semana 3: CSV Modular**.

### Objetivos de Aprendizaje Alcanzados

- âœ… Arquitectura modular y separaciÃ³n de responsabilidades
- âœ… Manejo profesional de configuraciÃ³n y logging
- âœ… Testing automatizado y TDD
- âœ… Procesamiento eficiente de datos
- âœ… ValidaciÃ³n y calidad de datos
- âœ… DocumentaciÃ³n tÃ©cnica completa

### Habilidades Desarrolladas

- DiseÃ±o de sistemas escalables
- Manejo avanzado de pandas y numpy
- Testing con pytest
- GestiÃ³n de configuraciÃ³n con archivos .env
- Logging estructurado
- Type hints y programaciÃ³n defensiva

## ğŸ“ Notas de VersiÃ³n

### v1.0.0 (Actual)

- âœ¨ Sistema completo de procesamiento CSV
- âœ¨ ValidaciÃ³n avanzada de datos
- âœ¨ ConfiguraciÃ³n flexible
- âœ¨ Suite completa de tests (74 tests)
- âœ¨ DocumentaciÃ³n profesional

## ğŸ¤ Soporte

Para reportar bugs o solicitar features:

1. Crear issue en el repositorio
2. Incluir logs y archivos de ejemplo
3. Describir comportamiento esperado vs actual

## ğŸ“š Referencias TÃ©cnicas

- **pandas**: ManipulaciÃ³n y anÃ¡lisis de datos
- **pytest**: Framework de testing
- **chardet**: DetecciÃ³n automÃ¡tica de encoding
- **python-dotenv**: GestiÃ³n de variables de entorno
- **logging**: Sistema de logs nativo de Python

---

**Desarrollado con â¤ï¸ como parte del Data + Automation Engineer Journey**

_Semana 3: Fundamentos de Python y Procesamiento Modular de Datos_
